---
title: "Capstone Project Milestone Report: Week 2"
author: "Marc Arroyo"
date: "24/2/2021"
output: html_document
---
## 0. Introduction

This document is Marc Arroyo's Week 2 Milestone Report of the **Capstone Project** of the **Data Science Specialization** offered by Johns Hopkins University via Coursera.

This Project will start with the basics, analyzing a large corpus of text documents to discover the structure in the data and how words are put together. It will cover cleaning and analyzing text data, then building and sampling from a predictive text model. Finally, you will use the knowledge you gained in data products to build a predictive text product.

## 1. Instructions

The goal of this report is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. 

Please submit a report that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. 

The motivation for this project is to: 

        1. Demonstrate that you've downloaded the data and have successfully loaded it in.
        2. Create a basic report of summary statistics about the data sets.
        3. Report any interesting findings that you amassed so far.
        4. Get feedback on your plans for creating a prediction algorithm and Shiny app. 

## 2. Exploring Data

### 2.1. Preparing Environment

First thing we will do is to load required libraries and set the echo parameter to TRUE in order to not specify it in every chunk. Libraries to use are: 

        - knitr in order to process the document and transform it to html
        - dplyr to manipulate dataframes
        - tm as the main text mining package in R
        - RWeka to use the tools of Weka written in Java
        - stringi for character string processing facilities
        - filesstrings to move files from folders
        - ggplot2 in order to create graphics
        - ggwordcloud to create wordclouds
        - gridExtra to arrange graphs
        - grid to put and format a title in the graphs grids
        - RColorBrewer to color our graphs
        
To set the echo parameter to TRUE we will use the opts_chunk$set instruction from knitr library. We will also indicate as a default option to not print messages or warnings in the final document and to stop in case of error in the code execution and not continue with the document.

We will also create a knitr hook to show formatted numbers in the text.

Finally we will use one code line to set the locale language to English, as mine it is not.

```{r, EnvPrep, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, results='hide'}

library(knitr)
library(dplyr)
library(tm)
library(RWeka)
library(stringi)
library(filesstrings)
library(ggplot2)
library(ggwordcloud)
library(gridExtra)
library(grid)
library(RColorBrewer)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)

knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",", decimal.mark = ".")
})

Sys.setlocale("LC_ALL", "English")

```

### 2.2. Loading data

To build our predictive text product we have been supplied with a dataset that can be found on the following link [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). In this link there are several files in four different languages, we will select only those related to English language.

Now we will download the dataset suposing we have set the working directory in the project directory.

```{r, DownloadData, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, results='hide'}

if(!file.exists("./data/en_US.twitter.txt")){
        
        # File doesn't already exists, download the file
        url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
        file <- "Coursera-SwiftKey.zip"
        download.file(url, file, method="curl")
        
        # Uncompress the file
        unzip(file)
        
        # Create data folder to store relevant files
        dir.create("./data")
        
        # Move English language files to the data folder and remove others
        file.move("./final/en_US/en_US.blogs.txt", "./data")
        file.move("./final/en_US/en_US.news.txt", "./data")
        file.move("./final/en_US/en_US.twitter.txt", "./data")
        
        unlink("Coursera-SwiftKey.zip", recursive = FALSE)
        unlink("./final", recursive = TRUE)
}

# Read and load files

TwitterFile <- readLines("./data/en_US.twitter.txt",encoding="UTF-8", 
                         skipNul = TRUE, warn = TRUE)

BlogsFile <- readLines("./data/en_US.blogs.txt",encoding="UTF-8", 
                         skipNul = TRUE, warn = TRUE)

NewsFile <- readLines("./data/en_US.news.txt",encoding="UTF-8", 
                         skipNul = TRUE, warn = TRUE)

```

### 2.3. Files Characterization

And once the three files are loaded, we will try to obtain their main characteristics.

```{r, CharacterizeFiles, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

# Files size

fsiztwt <- file.info("./data/en_US.twitter.txt")$size / 1024 ^ 2
fsizblo <- file.info("./data/en_US.blogs.txt")$size / 1024 ^ 2
fsiznew <- file.info("./data/en_US.news.txt")$size / 1024 ^ 2
fsiztot <- fsiztwt + fsizblo + fsiznew

# Number of lines of each file
tlintwt <- length(TwitterFile)
tlinblo <- length(BlogsFile)
tlinnew <- length(NewsFile)
tlintot <- tlintwt + tlinblo + tlinnew

# Number of words

twortwt <- sum(stri_count_words(TwitterFile))
tworblo <- sum(stri_count_words(BlogsFile))
twornew <- sum(stri_count_words(NewsFile))
twortot <- twortwt + tworblo + twornew

filesum <- data.frame(c("     en_US.twitter.txt", "     en_US.blogs.txt", "     en_US.news.txt", "TOTAL"),
                      c(fsiztwt, fsizblo, fsiznew, fsiztot),
                      c(tlintwt, tlinblo, tlinnew, tlintot),
                      c(twortwt, tworblo, twornew, twortot))

colnames(filesum) <- c("File Name", "File Size(Mb)", "Nb Lines", "Nb Words")

filesum <- filesum %>% mutate_if(is.numeric, prettyNum, big.mark = ",", decimal.mark = ".") 

kable(filesum, caption = "Files Information Summary")

```

As we can observe, Blogs and News files have similar size and similar number of lines, while twitter file has smaller size but more than the double of lines. In number of words, twitter file is the one with less words (30M words), and news the one with more (37M words).

## 3. Data Preprocessing

### 3.1. Creating a Sample

First thing we will do is to create a unique and more reduced file as the addition of a sample of all three. To create a sample of each file we will use a binomial function with the objective of approximately the 5% of each file.


```{r, SampleCreation, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

# Create a sample of 5% of lines using a binomial distribution

set.seed(1234)
TwitterSample <-  TwitterFile[rbinom(tlintwt, 1,  0.05)==1]
BlogsSample <-  BlogsFile[rbinom(tlinblo, 1,  0.05)==1]
NewsSample <-  NewsFile[rbinom(tlinnew, 1,  0.05)==1]

# Add the three samples in only one variable and free memory removing the
# three individual samples

TotSample <- c(BlogsSample, NewsSample, TwitterSample)
rm(BlogsSample, NewsSample, TwitterSample)

# Characterize our sample

tlinsamp <- length(TotSample)
tworsamp <- sum(stri_count_words(TotSample))

```

And now our sample data to work with is reduced to *`r tlinsamp`* lines and *`r tworsamp`* words, a lot of information but less than the total *`r tlintot`* lines and *`r twortot`* words of the three files together.

### 3.2. Creating and Preprocessing a Corpus

First we will do is apply a Profanity filter in order to skip words that we don't want to predict. To do so, we will download a publicly maintained [List of Dirty Naughty Obscene and Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en) and create a list of profanity words. With this list, using *setdiff* function we will eliminate this list of words from our sample, in order to not being predicted.

```{r, ProfanityFilter, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

profanity <- readLines("profanity_en.txt",encoding="UTF-8", 
                         skipNul = TRUE, warn = TRUE)

TotSample <- setdiff(x = TotSample, y = profanity)

tlinsampp <- length(TotSample)
tworsampp <- sum(stri_count_words(TotSample))
```

And now our sample data contains *`r tworsampp`* words, meaning that *`r tworsamp - tworsampp`* profanity words have been filtered .

Now, with the sample we created, we will generate a virtual corpus (hoping for enough memory available) and we will clean transform data a little bit.
The transformations we will do are:

        1. Transform the corpus to Plain Text
        2. Transform all letters to lower case in order to catch better word frequency
        3. Collapse multiple whitespaces in only one
        4. Remove numbers, as they are not words
        5. Remove all punctuation signs
        6. Remove special characters
        7. Remove stopwords
        
To accomplish this task we will use the tools provided by **[tm Package](https://cran.r-project.org/web/packages/tm/tm.pdf)**.

```{r, CorpusCreation, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

corpus <- VCorpus(VectorSource(TotSample))
corpus <- tm_map(corpus, content_transformer(PlainTextDocument))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
removeSC <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, removeSC, "/”|@|\\|")
corpus <- tm_map(corpus, removeWords, stopwords("en"))

```

## 4. Data Analysis

To understand a little bit the structure of our data we will tokenize our corpus using N-Grams, being N from 1 (individual words) to 4 (sequences of 4 words), and review the N-Grams of each type with more frequency.

### 4.1. Analyzing word by word

To transform our corpus in a "machine readable format" we will tokenize it using the tools offered by Weka via the RWeka package and tm package. Then we will extract the most used words and will be shown using a wordcloud and a bar graph for more clarity.

```{r, UniGram, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

WordTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, 
                                                              max = 1))
tdm.word = TermDocumentMatrix(corpus, 
                              control = list(tokenize = WordTokenizer))

freqWordTokenizer = sort(rowSums(as.matrix(removeSparseTerms(tdm.word, 0.997))),decreasing = TRUE)

freqWordTokenizer.df = data.frame(word=names(freqWordTokenizer), freq=freqWordTokenizer)

pal=brewer.pal(8,"Purples")
pal=pal[-(1:3)]
a <- ggwordcloud(freqWordTokenizer.df$word, freqWordTokenizer.df$freq,
                 max.words = 100, random.order = F, colors = pal)

b <- ggplot(head(freqWordTokenizer.df,15), aes(reorder(word,freq), freq)) +
        geom_bar(stat = "identity", fill = "purple") + coord_flip() +
        xlab("Bigrams") + ylab("Frequency") 

grid.arrange(a, b, nrow=1,
     top = textGrob("Most Frequent Bigrams",gp=gpar(fontsize=20,font=3)))

```

As we could expect, even removing stopwords, the most common words in our sample are the most common words in English and they provide little information.

For that reason, and trying to extract more information, we will repeat the analysis for 2,3 and 4th N-Grams.

### 4.2. Analyzing our sample by 2-Grams

To analyze our sample by NGrams, we will repeat the same procedure but changing Weka_control by our N, in case of 2-Grams 2.

The result will continue being shown using a wordcloud and a bar graph for better comparison.

```{r, BiGram, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, 
                                                              max = 2))
tdm.bigram = TermDocumentMatrix(corpus, 
                              control = list(tokenize = BiGramTokenizer))

freqBigramTokenizer = sort(rowSums(as.matrix(removeSparseTerms(tdm.bigram, 0.9998))),decreasing = TRUE)

freqBigramTokenizer.df = data.frame(word=names(freqBigramTokenizer), freq=freqBigramTokenizer)

pal=brewer.pal(8,"Blues")
pal=pal[-(1:3)]
a <- ggwordcloud(freqBigramTokenizer.df$word, freqBigramTokenizer.df$freq,
                 max.words = 100, random.order = F, colors = pal)

b <- ggplot(head(freqBigramTokenizer.df,15), aes(reorder(word,freq), freq)) +
        geom_bar(stat = "identity", fill = "blue") + coord_flip() +
        xlab("Bigrams") + ylab("Frequency") 

grid.arrange(a, b, nrow=1,
     top = textGrob("Most Frequent Bigrams",gp=gpar(fontsize=20,font=3)))

```

Result seems quite logical, with really common expressions mainly around timing, but also around personal actions or feelings and, the city of New York.

### 4.3. Analyzing our sample by 3-Grams

Once again, we will repeat the same procedure but now N will be 3.

```{r, TriGram, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

TriGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, 
                                                              max = 3))
tdm.trigram = TermDocumentMatrix(corpus, 
                              control = list(tokenize = TriGramTokenizer))

freqTrigramTokenizer = sort(rowSums(as.matrix(removeSparseTerms(tdm.trigram, 0.9999))),decreasing = TRUE)

freqTrigramTokenizer.df = data.frame(word=names(freqTrigramTokenizer), freq=freqTrigramTokenizer)

pal=brewer.pal(8,"Reds")
pal=pal[-(1:3)]
a <- ggwordcloud(freqTrigramTokenizer.df$word, freqTrigramTokenizer.df$freq,
                 max.words = 100, random.order = F, colors = pal)

b <- ggplot(head(freqTrigramTokenizer.df,15), aes(reorder(word,freq), freq)) +
        geom_bar(stat = "identity", fill = "red") + coord_flip() +
        xlab("Trigrams") + ylab("Frequency")

grid.arrange(a, b, nrow=1,
     top = textGrob("Most Frequent Trigrams",gp=gpar(fontsize=20,font=3)))

```

In trigrams, appear on the top congratulations for special calendar days like mothers day or new year, NY city stays there and doubles its presence with NY Times, and also stay there some expressions around timming and President Barack Obama joins the list. 

### 4.4. Analyzing our sample by 4-Grams

And finally with N = 4 to analyze the QuadGrams.

```{r, QuadGram, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, inline = TRUE}

QuadGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, 
                                                              max = 4))
tdm.quadgram = TermDocumentMatrix(corpus, 
                              control = list(tokenize = QuadGramTokenizer))

freqQuadgramTokenizer = sort(rowSums(as.matrix(removeSparseTerms(tdm.quadgram, 0.99998))),decreasing = TRUE)

freqQuadgramTokenizer.df = data.frame(word=names(freqQuadgramTokenizer), freq=freqQuadgramTokenizer)

pal=brewer.pal(8,"Greens")
pal=pal[-(1:3)]
a <- ggwordcloud(freqQuadgramTokenizer.df$word, freqQuadgramTokenizer.df$freq,
                 max.words = 100, random.order = F, colors = pal)

b <- ggplot(head(freqQuadgramTokenizer.df,15), aes(reorder(word,freq), freq)) +
        geom_bar(stat = "identity", fill = "green") + coord_flip() +
        xlab("Quadgrams") + ylab("Frequency")

grid.arrange(a, b, nrow=1,
     top = textGrob("Most Frequent Quadgrams",gp=gpar(fontsize=20,font=3)))

```

In quadgrams, one relevant thing can be observed. 

A lot of the most common quadgrams (food composition related) are formed because we removed some characters and words from our sample. For example, the most repeated quadgram is "g fat g saturated", and clearly refers to the number of grams of fat and number of them that are saturated, but we removed numbers and stopwords, being the real expression something like "15 g fat and 7g of them saturated", but "and", "of" and "them" are stopwords and the numbers have been removed.

For this reason, the validity of quadgrams to create predictions has to be evaluated.
  
## 5. Conclusions and Next Steps

Some conclusions have been extracted and need to be evaluated:

        - Due to processing capabilities sample has been reduced to 5%, and we have removed sparse terms adjusting sparsity to the maximum that our RAM memory could manage. Hoping this limitations won't affect predictions.
        - Bigrams and Trigrams seems to be useful to predict
        - Due to the removal of stopwords and numbers, quadgrams create some doubts of their utility for prediction and needs to be further investigated.

Next steps will be:

        1. Reevaluate the treatment done to the sample in order to see if towards the prediction accuracy is better to include or not stopwords and numbers
        2. Create our predicting model
        3. Create our Shiny app using the model build